{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de YOLO sur raspberry pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation des dÃ©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics[export] in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (8.3.38)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (3.9.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (6.1.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (2.0.12)\n",
      "Requirement already satisfied: onnx>=1.12.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (1.17.0)\n",
      "Requirement already satisfied: coremltools>=7.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (8.1)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (1.6.0)\n",
      "Requirement already satisfied: openvino>=2024.0.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (2024.5.0)\n",
      "Requirement already satisfied: tensorflow>=2.0.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (2.18.0)\n",
      "Requirement already satisfied: tensorflowjs>=3.9.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (4.22.0)\n",
      "Requirement already satisfied: keras in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from ultralytics[export]) (3.7.0)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from coremltools>=7.0->ultralytics[export]) (3.20.3)\n",
      "Requirement already satisfied: sympy in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from coremltools>=7.0->ultralytics[export]) (1.13.1)\n",
      "Requirement already satisfied: packaging in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from coremltools>=7.0->ultralytics[export]) (23.2)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from coremltools>=7.0->ultralytics[export]) (24.2.0)\n",
      "Requirement already satisfied: cattrs in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from coremltools>=7.0->ultralytics[export]) (24.1.2)\n",
      "Requirement already satisfied: pyaml in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from coremltools>=7.0->ultralytics[export]) (24.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics[export]) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics[export]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics[export]) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics[export]) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics[export]) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics[export]) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics[export]) (6.4.5)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.2.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from openvino>=2024.0.0->ultralytics[export]) (2024.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics[export]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics[export]) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics[export]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics[export]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics[export]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics[export]) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from scikit-learn>=1.3.2->ultralytics[export]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from scikit-learn>=1.3.2->ultralytics[export]) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (2.18.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow>=2.0.0->ultralytics[export]) (0.37.1)\n",
      "Requirement already satisfied: rich in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from keras->ultralytics[export]) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from keras->ultralytics[export]) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from keras->ultralytics[export]) (0.13.1)\n",
      "Requirement already satisfied: flax>=0.7.2 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflowjs>=3.9.0->ultralytics[export]) (0.8.5)\n",
      "Requirement already satisfied: jax>=0.4.13 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflowjs>=3.9.0->ultralytics[export]) (0.4.30)\n",
      "Requirement already satisfied: jaxlib>=0.4.13 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflowjs>=3.9.0->ultralytics[export]) (0.4.30)\n",
      "Requirement already satisfied: tf-keras>=2.13.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflowjs>=3.9.0->ultralytics[export]) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflowjs>=3.9.0->ultralytics[export]) (1.11.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.16.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflowjs>=3.9.0->ultralytics[export]) (0.16.1)\n",
      "Requirement already satisfied: filelock in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics[export]) (3.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from sympy->coremltools>=7.0->ultralytics[export]) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->ultralytics[export]) (0.44.0)\n",
      "Requirement already satisfied: msgpack in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (1.1.0)\n",
      "Requirement already satisfied: optax in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (0.2.4)\n",
      "Requirement already satisfied: orbax-checkpoint in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (0.6.4)\n",
      "Requirement already satisfied: tensorstore in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (0.1.69)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics[export]) (3.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from jax>=0.4.13->tensorflowjs>=3.9.0->ultralytics[export]) (8.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from rich->keras->ultralytics[export]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from rich->keras->ultralytics[export]) (2.18.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.0.0->ultralytics[export]) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.0.0->ultralytics[export]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow>=2.0.0->ultralytics[export]) (3.1.3)\n",
      "Requirement already satisfied: wurlitzer in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=3.9.0->ultralytics[export]) (3.1.1)\n",
      "Requirement already satisfied: ydf in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs>=3.9.0->ultralytics[export]) (0.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.1.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from cattrs->coremltools>=7.0->ultralytics[export]) (1.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from jinja2->torch>=1.8.0->ultralytics[export]) (3.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras->ultralytics[export]) (0.1.2)\n",
      "Requirement already satisfied: chex>=0.1.87 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from optax->flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (0.1.87)\n",
      "Requirement already satisfied: etils[epy] in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from optax->flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (1.5.2)\n",
      "Requirement already satisfied: nest_asyncio in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (1.6.0)\n",
      "Requirement already satisfied: humanize in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (4.11.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs>=3.9.0->ultralytics[export]) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics[export]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des dÃ©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.benchmarks import benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export des modÃ¨les sous diffÃ©rents formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model_name: str = \"yolo11n\", format: str = \"OpenVINO\", device : str = \"cpu\", image_size : int = 640) -> str:\n",
    "    \"\"\"\n",
    "    Exportation du modÃ¨le YOLO dans un certain format.\n",
    "\n",
    "    :param model_name : Nom du modÃ¨le Ã  exporter.\n",
    "    :param format : Format d'exportation du modÃ¨le.\n",
    "    :param device : PÃ©riphÃ©rique sur lequel le modÃ¨le doit Ãªtre exportÃ© (GPU -> 0, CPU -> cpu, NVDIA Jetson -> dla:0 / dla:1).\n",
    "    :param image_size : Taille de l'image d'entrÃ©e du modÃ¨le.\n",
    "    \"\"\"\n",
    "\n",
    "    model = YOLO(model_name)\n",
    "\n",
    "    return model.export(format=format, device=device, imgsz=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark(model=\"yolo11n.pt\", data=\"coco8.yaml\", imgsz=640, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formats = [\"torchscript\", \"onnx\", \"openvino\", \"saved_model\", \"pb\", \"tflite\", \"paddle\", \"mnn\", \"ncnn\"]\n",
    "models = [\"yolo11n\", \"yolo11s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 5.95MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.5.1+cu124...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 1.9s, saved as 'yolo11n.torchscript' (10.5 MB)\n",
      "\n",
      "Export complete (2.5s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n.torchscript imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.5.1+cu124...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 1.8s, saved as 'yolo11n.torchscript' (10.6 MB)\n",
      "\n",
      "Export complete (2.2s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n.torchscript imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.6s, saved as 'yolo11n.onnx' (10.2 MB)\n",
      "\n",
      "Export complete (1.9s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n.onnx imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2024.5.0-17288-7975fa5da0c-refs/pull/3856/head...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success âœ… 4.1s, saved as 'yolo11n_openvino_model/' (10.4 MB)\n",
      "\n",
      "Export complete (4.5s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n_openvino_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n_openvino_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733845558.354709   21820 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733845558.368836   21820 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/calibration_image_sample_data_20x128x128x3_float32.npy.zip to 'calibration_image_sample_data_20x128x128x3_float32.npy.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.11M/1.11M [00:00<00:00, 9.37MB/s]\n",
      "Unzipping calibration_image_sample_data_20x128x128x3_float32.npy.zip to /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/calibration_image_sample_data_20x128x128x3_float32.npy...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 55.78file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.2s, saved as 'yolo11n.onnx' (10.2 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.20.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733845573.697266   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733845573.697400   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845574.664178   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845574.664212   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1733845575.742954   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845575.743037   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845576.719665   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845576.719682   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 20.7s, saved as 'yolo11n_saved_model' (25.8 MB)\n",
      "\n",
      "Export complete (21.2s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n_saved_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n_saved_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.2s, saved as 'yolo11n.onnx' (10.2 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.20.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733845589.229054   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845589.229155   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845590.043137   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845590.043153   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1733845591.034477   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845591.034562   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845591.805560   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845591.805576   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 14.7s, saved as 'yolo11n_saved_model' (25.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow GraphDef:\u001b[0m starting export with tensorflow 2.18.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733845594.266994   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845594.267186   21820 single_machine.cc:361] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow GraphDef:\u001b[0m export success âœ… 1.7s, saved as 'yolo11n.pb' (10.3 MB)\n",
      "\n",
      "Export complete (16.8s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n.pb imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n.pb imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.1s, saved as 'yolo11n.onnx' (10.3 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.20.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733845606.440556   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845606.440665   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845607.300359   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845607.300375   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1733845608.331452   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845608.331544   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845609.133785   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845609.133804   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 14.9s, saved as 'yolo11n_saved_model' (26.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 0.0s, saved as 'yolo11n_saved_model/yolo11n_float32.tflite' (10.3 MB)\n",
      "\n",
      "Export complete (15.5s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n_saved_model/yolo11n_float32.tflite imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n_saved_model/yolo11n_float32.tflite imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mPaddlePaddle:\u001b[0m starting export with X2Paddle 1.5.0...\n",
      "Exporting inference model from python code ('/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/yolo11n_paddle_model/x2paddle_code.py')... \n",
      "\n",
      "\u001b[34m\u001b[1mPaddlePaddle:\u001b[0m export success âœ… 5.3s, saved as 'yolo11n_paddle_model/' (20.5 MB)\n",
      "\n",
      "Export complete (5.7s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n_paddle_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n_paddle_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 16:46:56.509186 21820 program_interpreter.cc:212] New Executor is Running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.3s, saved as 'yolo11n.onnx' (10.2 MB)\n",
      "CPU Group: [ 6  4  2  0  7  5  3  1 ], 800000 - 4000000\n",
      "The device supports: i8sdot:0, fp16:0, i8mm: 0, sve2: 0\n",
      "CPU Group: [ 6  4  2  0  7  5  3  1 ], 800000 - 4000000\n",
      "The device supports: i8sdot:0, fp16:0, i8mm: 0, sve2: 0\n",
      "\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m starting export with MNN 3.0.1...\n",
      "Start to Convert Other Model Format To MNN Model..., target version: \u001b[34m\u001b[1mMNN:\u001b[0m export success âœ… 6.8s, saved as 'yolo11n.mnn' (10.1 MB)\n",
      "3\n",
      "[16:47:03] :46: ONNX Model ir version: 9\n",
      "[16:47:03] :47: ONNX Model opset version: 19\n",
      "[16:47:03] :146: Check it out ==> /model.11/Resize_output_0 has empty input, the index is 1\n",
      "[16:47:03] :146: Check it out ==> /model.14/Resize_output_0 has empty input, the index is 1\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ images, ]\n",
      "outputTensors: [ output0, ]\n",
      "Converted Success!\n",
      "\n",
      "Export complete (7.1s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n.mnn imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n.mnn imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.5.1+cu124...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 2.1s, saved as 'yolo11n.torchscript' (10.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m starting export with NCNN 1.0.20240820...\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m running '/home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages/ultralytics/pnnx yolo11n.torchscript ncnnparam=yolo11n_ncnn_model/model.ncnn.param ncnnbin=yolo11n_ncnn_model/model.ncnn.bin ncnnpy=yolo11n_ncnn_model/model_ncnn.py pnnxparam=yolo11n_ncnn_model/model.pnnx.param pnnxbin=yolo11n_ncnn_model/model.pnnx.bin pnnxpy=yolo11n_ncnn_model/model_pnnx.py pnnxonnx=yolo11n_ncnn_model/model.pnnx.onnx fp16=0 device=cpu inputshape=\"[1, 3, 640, 640]\"'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pnnxparam = yolo11n_ncnn_model/model.pnnx.param\n",
      "pnnxbin = yolo11n_ncnn_model/model.pnnx.bin\n",
      "pnnxpy = yolo11n_ncnn_model/model_pnnx.py\n",
      "pnnxonnx = yolo11n_ncnn_model/model.pnnx.onnx\n",
      "ncnnparam = yolo11n_ncnn_model/model.ncnn.param\n",
      "ncnnbin = yolo11n_ncnn_model/model.ncnn.bin\n",
      "ncnnpy = yolo11n_ncnn_model/model_ncnn.py\n",
      "fp16 = 0\n",
      "optlevel = 2\n",
      "device = cpu\n",
      "inputshape = [1,3,640,640]f32\n",
      "inputshape2 = \n",
      "customop = \n",
      "moduleop = \n",
      "############# pass_level0\n",
      "inline module = torch.nn.modules.linear.Identity\n",
      "inline module = ultralytics.nn.modules.block.Attention\n",
      "inline module = ultralytics.nn.modules.block.Bottleneck\n",
      "inline module = ultralytics.nn.modules.block.C2PSA\n",
      "inline module = ultralytics.nn.modules.block.C3k\n",
      "inline module = ultralytics.nn.modules.block.C3k2\n",
      "inline module = ultralytics.nn.modules.block.DFL\n",
      "inline module = ultralytics.nn.modules.block.PSABlock\n",
      "inline module = ultralytics.nn.modules.block.SPPF\n",
      "inline module = ultralytics.nn.modules.conv.Concat\n",
      "inline module = ultralytics.nn.modules.conv.Conv\n",
      "inline module = ultralytics.nn.modules.conv.DWConv\n",
      "inline module = ultralytics.nn.modules.head.Detect\n",
      "inline module = torch.nn.modules.linear.Identity\n",
      "inline module = ultralytics.nn.modules.block.Attention\n",
      "inline module = ultralytics.nn.modules.block.Bottleneck\n",
      "inline module = ultralytics.nn.modules.block.C2PSA\n",
      "inline module = ultralytics.nn.modules.block.C3k\n",
      "inline module = ultralytics.nn.modules.block.C3k2\n",
      "inline module = ultralytics.nn.modules.block.DFL\n",
      "inline module = ultralytics.nn.modules.block.PSABlock\n",
      "inline module = ultralytics.nn.modules.block.SPPF\n",
      "inline module = ultralytics.nn.modules.conv.Concat\n",
      "inline module = ultralytics.nn.modules.conv.Conv\n",
      "inline module = ultralytics.nn.modules.conv.DWConv\n",
      "inline module = ultralytics.nn.modules.head.Detect\n",
      "\n",
      "----------------\n",
      "\n",
      "############# pass_level1\n",
      "############# pass_level2\n",
      "############# pass_level3\n",
      "############# pass_level4\n",
      "############# pass_level5\n",
      "############# pass_ncnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mNCNN:\u001b[0m export success âœ… 2.0s, saved as 'yolo11n_ncnn_model' (10.2 MB)\n",
      "\n",
      "Export complete (4.4s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n_ncnn_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n_ncnn_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt to 'yolo11s.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18.4M/18.4M [00:03<00:00, 6.34MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.5.1+cu124...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 2.5s, saved as 'yolo11s.torchscript' (36.6 MB)\n",
      "\n",
      "Export complete (3.2s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s.torchscript imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.5.1+cu124...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 2.5s, saved as 'yolo11s.torchscript' (36.6 MB)\n",
      "\n",
      "Export complete (3.1s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s.torchscript imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.9s, saved as 'yolo11s.onnx' (36.3 MB)\n",
      "\n",
      "Export complete (2.8s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s.onnx imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m starting export with openvino 2024.5.0-17288-7975fa5da0c-refs/pull/3856/head...\n",
      "\u001b[34m\u001b[1mOpenVINO:\u001b[0m export success âœ… 4.4s, saved as 'yolo11s_openvino_model/' (36.4 MB)\n",
      "\n",
      "Export complete (5.2s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s_openvino_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s_openvino_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.6s, saved as 'yolo11s.onnx' (36.3 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.20.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733845660.138146   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845660.138246   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845661.596316   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845661.596334   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1733845663.444782   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845663.444872   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845664.981735   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845664.981761   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 20.1s, saved as 'yolo11s_saved_model' (91.1 MB)\n",
      "\n",
      "Export complete (20.8s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s_saved_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s_saved_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.6s, saved as 'yolo11s.onnx' (36.3 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.20.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733845680.380672   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845680.380773   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845681.565914   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845681.565929   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1733845683.249199   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845683.249293   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845684.389729   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845684.389743   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 18.3s, saved as 'yolo11s_saved_model' (91.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow GraphDef:\u001b[0m starting export with tensorflow 2.18.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733845688.090632   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845688.090718   21820 single_machine.cc:361] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow GraphDef:\u001b[0m export success âœ… 1.9s, saved as 'yolo11s.pb' (36.4 MB)\n",
      "\n",
      "Export complete (20.8s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s.pb imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s.pb imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.5s, saved as 'yolo11s.onnx' (36.4 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.20.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733845701.234066   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845701.234208   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845702.821966   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845702.821980   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1733845704.642489   21820 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1733845704.642585   21820 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1733845705.867259   21820 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733845705.867277   21820 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 19.0s, saved as 'yolo11s_saved_model' (91.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 0.0s, saved as 'yolo11s_saved_model/yolo11s_float32.tflite' (36.4 MB)\n",
      "\n",
      "Export complete (20.0s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s_saved_model/yolo11s_float32.tflite imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s_saved_model/yolo11s_float32.tflite imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mPaddlePaddle:\u001b[0m starting export with X2Paddle 1.5.0...\n",
      "Exporting inference model from python code ('/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/yolo11s_paddle_model/x2paddle_code.py')... \n",
      "\n",
      "\u001b[34m\u001b[1mPaddlePaddle:\u001b[0m export success âœ… 5.3s, saved as 'yolo11s_paddle_model/' (72.5 MB)\n",
      "\n",
      "Export complete (6.2s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s_paddle_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s_paddle_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.42...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.5s, saved as 'yolo11s.onnx' (36.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mMNN:\u001b[0m starting export with MNN 3.0.1...\n",
      "Start to Convert Other Model Format To MNN Model..., target version: \u001b[34m\u001b[1mMNN:\u001b[0m export success âœ… 1.9s, saved as 'yolo11s.mnn' (36.2 MB)\n",
      "3\n",
      "[16:48:37] :46: ONNX Model ir version: 9\n",
      "[16:48:37] :47: ONNX Model opset version: 19\n",
      "[16:48:37] :146: Check it out ==> /model.11/Resize_output_0 has empty input, the index is 1\n",
      "[16:48:37] :146: Check it out ==> /model.14/Resize_output_0 has empty input, the index is 1\n",
      "Start to Optimize the MNN Net...\n",
      "inputTensors : [ images, ]\n",
      "outputTensors: [ output0, ]\n",
      "Converted Success!\n",
      "\n",
      "Export complete (2.5s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s.mnn imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s.mnn imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Ultralytics 8.3.38 ðŸš€ Python-3.9.20 torch-2.5.1+cu124 CPU (Intel Core(TM) i5-8300H 2.30GHz)\n",
      "YOLO11s summary (fused): 238 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.5.1+cu124...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 2.2s, saved as 'yolo11s.torchscript' (36.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m starting export with NCNN 1.0.20240820...\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m running '/home/jake/miniconda3/envs/projet_developpement_application_IA_embarquee/lib/python3.9/site-packages/ultralytics/pnnx yolo11s.torchscript ncnnparam=yolo11s_ncnn_model/model.ncnn.param ncnnbin=yolo11s_ncnn_model/model.ncnn.bin ncnnpy=yolo11s_ncnn_model/model_ncnn.py pnnxparam=yolo11s_ncnn_model/model.pnnx.param pnnxbin=yolo11s_ncnn_model/model.pnnx.bin pnnxpy=yolo11s_ncnn_model/model_pnnx.py pnnxonnx=yolo11s_ncnn_model/model.pnnx.onnx fp16=0 device=cpu inputshape=\"[1, 3, 640, 640]\"'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pnnxparam = yolo11s_ncnn_model/model.pnnx.param\n",
      "pnnxbin = yolo11s_ncnn_model/model.pnnx.bin\n",
      "pnnxpy = yolo11s_ncnn_model/model_pnnx.py\n",
      "pnnxonnx = yolo11s_ncnn_model/model.pnnx.onnx\n",
      "ncnnparam = yolo11s_ncnn_model/model.ncnn.param\n",
      "ncnnbin = yolo11s_ncnn_model/model.ncnn.bin\n",
      "ncnnpy = yolo11s_ncnn_model/model_ncnn.py\n",
      "fp16 = 0\n",
      "optlevel = 2\n",
      "device = cpu\n",
      "inputshape = [1,3,640,640]f32\n",
      "inputshape2 = \n",
      "customop = \n",
      "moduleop = \n",
      "############# pass_level0\n",
      "inline module = torch.nn.modules.linear.Identity\n",
      "inline module = ultralytics.nn.modules.block.Attention\n",
      "inline module = ultralytics.nn.modules.block.Bottleneck\n",
      "inline module = ultralytics.nn.modules.block.C2PSA\n",
      "inline module = ultralytics.nn.modules.block.C3k\n",
      "inline module = ultralytics.nn.modules.block.C3k2\n",
      "inline module = ultralytics.nn.modules.block.DFL\n",
      "inline module = ultralytics.nn.modules.block.PSABlock\n",
      "inline module = ultralytics.nn.modules.block.SPPF\n",
      "inline module = ultralytics.nn.modules.conv.Concat\n",
      "inline module = ultralytics.nn.modules.conv.Conv\n",
      "inline module = ultralytics.nn.modules.conv.DWConv\n",
      "inline module = ultralytics.nn.modules.head.Detect\n",
      "inline module = torch.nn.modules.linear.Identity\n",
      "inline module = ultralytics.nn.modules.block.Attention\n",
      "inline module = ultralytics.nn.modules.block.Bottleneck\n",
      "inline module = ultralytics.nn.modules.block.C2PSA\n",
      "inline module = ultralytics.nn.modules.block.C3k\n",
      "inline module = ultralytics.nn.modules.block.C3k2\n",
      "inline module = ultralytics.nn.modules.block.DFL\n",
      "inline module = ultralytics.nn.modules.block.PSABlock\n",
      "inline module = ultralytics.nn.modules.block.SPPF\n",
      "inline module = ultralytics.nn.modules.conv.Concat\n",
      "inline module = ultralytics.nn.modules.conv.Conv\n",
      "inline module = ultralytics.nn.modules.conv.DWConv\n",
      "inline module = ultralytics.nn.modules.head.Detect\n",
      "\n",
      "----------------\n",
      "\n",
      "############# pass_level1\n",
      "############# pass_level2\n",
      "############# pass_level3\n",
      "############# pass_level4\n",
      "############# pass_level5\n",
      "############# pass_ncnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mNCNN:\u001b[0m export success âœ… 3.2s, saved as 'yolo11s_ncnn_model' (36.2 MB)\n",
      "\n",
      "Export complete (6.0s)\n",
      "Results saved to \u001b[1m/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s_ncnn_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11s_ncnn_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "Exportation terminÃ©e.\n"
     ]
    }
   ],
   "source": [
    "models_path = []\n",
    "\n",
    "for model in models:\n",
    "    for format in formats:\n",
    "        models_path.append(export_model(model_name=model, format=format))\n",
    "\n",
    "print(\"Exportation terminÃ©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yolo11n.torchscript',\n",
       " 'yolo11n.torchscript',\n",
       " 'yolo11n.onnx',\n",
       " 'yolo11n_openvino_model',\n",
       " 'yolo11n_saved_model',\n",
       " 'yolo11n.pb',\n",
       " 'yolo11n_saved_model/yolo11n_float32.tflite',\n",
       " 'yolo11n_paddle_model',\n",
       " 'yolo11n.mnn',\n",
       " 'yolo11n_ncnn_model',\n",
       " 'yolo11s.torchscript',\n",
       " 'yolo11s.torchscript',\n",
       " 'yolo11s.onnx',\n",
       " 'yolo11s_openvino_model',\n",
       " 'yolo11s_saved_model',\n",
       " 'yolo11s.pb',\n",
       " 'yolo11s_saved_model/yolo11s_float32.tflite',\n",
       " 'yolo11s_paddle_model',\n",
       " 'yolo11s.mnn',\n",
       " 'yolo11s_ncnn_model']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModÃ¨le : yolo11n.torchscript\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n.torchscript for TorchScript inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 248.3ms\n",
      "Speed: 23.2ms preprocess, 248.3ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 23.171186447143555, 'inference': 248.34585189819336, 'postprocess': 11.376142501831055}]\n",
      "\n",
      "ModÃ¨le : yolo11n.torchscript\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n.torchscript for TorchScript inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 225.1ms\n",
      "Speed: 2.6ms preprocess, 225.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 2.6345252990722656, 'inference': 225.10123252868652, 'postprocess': 0.9667873382568359}]\n",
      "\n",
      "ModÃ¨le : yolo11n.onnx\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n.onnx for ONNX Runtime inference...\n",
      "Preferring ONNX Runtime TensorrtExecutionProvider\n",
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:490 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CPUExecutionProvider'] and retrying.\n",
      "****************************************\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2024-12-10 17:03:21.583899260 [E:onnxruntime:Default, provider_bridge_ort.cc:1978 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1637 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 79.6ms\n",
      "Speed: 2.7ms preprocess, 79.6ms inference, 30.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 2.742767333984375, 'inference': 79.62775230407715, 'postprocess': 30.575275421142578}]\n",
      "\n",
      "ModÃ¨le : yolo11n_openvino_model\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 47.2ms\n",
      "Speed: 3.3ms preprocess, 47.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 3.2587051391601562, 'inference': 47.1653938293457, 'postprocess': 0.9660720825195312}]\n",
      "\n",
      "ModÃ¨le : yolo11n_saved_model\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n_saved_model for TensorFlow SavedModel inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 427.2ms\n",
      "Speed: 2.7ms preprocess, 427.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 2.6984214782714844, 'inference': 427.2301197052002, 'postprocess': 0.9629726409912109}]\n",
      "\n",
      "ModÃ¨le : yolo11n.pb\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n.pb for TensorFlow GraphDef inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 460.9ms\n",
      "Speed: 6.7ms preprocess, 460.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 6.661176681518555, 'inference': 460.8776569366455, 'postprocess': 1.0733604431152344}]\n",
      "\n",
      "ModÃ¨le : yolo11n_saved_model/yolo11n_float32.tflite\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n_saved_model/yolo11n_float32.tflite for TensorFlow Lite inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 138.9ms\n",
      "Speed: 4.0ms preprocess, 138.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 4.005908966064453, 'inference': 138.86189460754395, 'postprocess': 0.9784698486328125}]\n",
      "\n",
      "ModÃ¨le : yolo11n_paddle_model\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n_paddle_model for PaddlePaddle inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1210 17:03:24.559384 21820 analysis_predictor.cc:1626] MKLDNN is enabled\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_build_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_analysis_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [mkldnn_placement_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [simplify_with_basic_ops_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [layer_norm_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [attention_lstm_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [seqconv_eltadd_relu_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [seqpool_cvm_concat_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [mul_lstm_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [fc_gru_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [mul_gru_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [seq_concat_fc_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_squeeze2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_reshape2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_flatten2_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [matmul_v2_scale_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_mul_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_v2_to_matmul_pass]\u001b[0m\n",
      "I1210 17:03:24.634800 21820 fuse_pass_base.cc:59] ---  detected 2 subgraphs\n",
      "\u001b[32m--- Running IR pass [matmul_scale_fuse_pass]\u001b[0m\n",
      "I1210 17:03:24.636039 21820 fuse_pass_base.cc:59] ---  detected 1 subgraphs\n",
      "\u001b[32m--- Running IR pass [gpu_cpu_map_matmul_to_mul_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [fc_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [repeated_fc_relu_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [squared_mat_sub_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_transpose_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [is_test_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [constant_folding_pass]\u001b[0m\n",
      "I1210 17:03:24.721163 21820 fuse_pass_base.cc:59] ---  detected 87 subgraphs\n",
      "\u001b[32m--- Running IR pass [squeeze2_transpose2_onednn_fuse_pass]\u001b[0m\n",
      "\u001b[37m--- fused 0 squeeze2 with transpose2\u001b[0m\n",
      "\u001b[32m--- Running IR pass [depthwise_conv_mkldnn_pass]\u001b[0m\n",
      "I1210 17:03:24.722716 21820 fuse_pass_base.cc:59] ---  detected 7 subgraphs\n",
      "\u001b[32m--- Running IR pass [conv_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_eltwiseadd_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_affine_channel_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_transpose_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_transpose_eltwiseadd_bn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_bias_mkldnn_fuse_pass]\u001b[0m\n",
      "W1210 17:03:24.805791 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.805867 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.805933 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.805999 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806063 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806128 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806193 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806258 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806322 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806391 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806455 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806522 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806587 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806651 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806715 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806780 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806844 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806908 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.806972 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807037 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807101 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807166 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807230 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807294 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807363 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807426 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807492 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807556 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807621 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807711 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807801 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807896 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.807993 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808094 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808192 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808363 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808434 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808498 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808566 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808631 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808696 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808763 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808830 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808894 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.808959 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809024 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809089 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809154 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809219 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809285 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809350 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809422 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809489 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809553 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809619 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809710 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809805 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.809908 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810009 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810106 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810206 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810299 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810408 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810521 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810628 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810734 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810834 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.810936 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811048 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811151 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811245 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811350 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811455 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811554 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811648 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811717 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811782 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811847 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811913 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.811978 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.812043 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.812109 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.812204 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.812275 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.812345 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.812456 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "W1210 17:03:24.812527 21820 op_compat_sensible_pass.cc:232]  Check the Attr(axis) of Op(elementwise_add) in pass(conv_bias_mkldnn_fuse_pass) failed!\n",
      "\u001b[32m--- Running IR pass [conv_transpose_bias_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_elementwise_add_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [conv_activation_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [scale_matmul_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [reshape_transpose_matmul_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [matmul_transpose_reshape_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [matmul_elementwise_add_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [matmul_activation_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [fc_mkldnn_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [fc_act_mkldnn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [self_attention_fuse_pass]\u001b[0m\n",
      "\u001b[37m---    fused 0 self attention (of scaled_dp_attention) with self_attention_fuse\u001b[0m\n",
      "\u001b[32m--- Running IR pass [batch_norm_act_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [softplus_activation_onednn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [shuffle_channel_mkldnn_detect_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [elementwise_act_onednn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [operator_scale_onednn_fuse_pass]\u001b[0m\n",
      "I1210 17:03:25.452920 21820 fuse_pass_base.cc:59] ---  detected 1 subgraphs\n",
      "\u001b[37m---    fused 1 elementwise_add with scale\u001b[0m\n",
      "\u001b[32m--- Running IR pass [operator_unsqueeze2_onednn_fuse_pass]\u001b[0m\n",
      "\u001b[32m--- Running IR pass [operator_reshape2_onednn_fuse_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [save_optimized_model_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_params_sync_among_devices_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [adjust_cudnn_workspace_size_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [inference_op_replace_pass]\u001b[0m\n",
      "\u001b[1m\u001b[35m--- Running analysis [ir_graph_to_program_pass]\u001b[0m\n",
      "I1210 17:03:25.495918 21820 analysis_predictor.cc:1838] ======= optimize end =======\n",
      "I1210 17:03:25.497565 21820 naive_executor.cc:200] ---  skip [feed], feed -> x0\n",
      "I1210 17:03:25.499814 21820 naive_executor.cc:200] ---  skip [save_infer_model/scale_0.tmp_0], fetch -> fetch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 377.1ms\n",
      "Speed: 2.9ms preprocess, 377.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 2.8569698333740234, 'inference': 377.07066535949707, 'postprocess': 0.8187294006347656}]\n",
      "\n",
      "ModÃ¨le : yolo11n.mnn\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n.mnn for MNN inference...\n",
      "MNN use low precision\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 69.1ms\n",
      "Speed: 5.7ms preprocess, 69.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 5.6705474853515625, 'inference': 69.12469863891602, 'postprocess': 1.081228256225586}]\n",
      "\n",
      "ModÃ¨le : yolo11n_ncnn_model\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n_ncnn_model for NCNN inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 85.9ms\n",
      "Speed: 3.2ms preprocess, 85.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 3.2253265380859375, 'inference': 85.94894409179688, 'postprocess': 1.7306804656982422}]\n",
      "\n",
      "ModÃ¨le : yolo11s.torchscript\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11s.torchscript for TorchScript inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 299.0ms\n",
      "Speed: 3.2ms preprocess, 299.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 3.2334327697753906, 'inference': 299.00455474853516, 'postprocess': 0.9622573852539062}]\n",
      "\n",
      "ModÃ¨le : yolo11s.torchscript\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11s.torchscript for TorchScript inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 259.0ms\n",
      "Speed: 2.6ms preprocess, 259.0ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 2.6493072509765625, 'inference': 259.0174674987793, 'postprocess': 0.8957386016845703}]\n",
      "\n",
      "ModÃ¨le : yolo11s.onnx\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11s.onnx for ONNX Runtime inference...\n",
      "Preferring ONNX Runtime TensorrtExecutionProvider\n",
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:490 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CPUExecutionProvider'] and retrying.\n",
      "****************************************\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2024-12-10 17:03:27.132183153 [E:onnxruntime:Default, provider_bridge_ort.cc:1978 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1637 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 146.1ms\n",
      "Speed: 2.8ms preprocess, 146.1ms inference, 41.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 2.7942657470703125, 'inference': 146.12364768981934, 'postprocess': 41.9464111328125}]\n",
      "\n",
      "ModÃ¨le : yolo11s_openvino_model\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11s_openvino_model for OpenVINO inference...\n",
      "Using OpenVINO LATENCY mode for batch=1 inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 142.0ms\n",
      "Speed: 3.0ms preprocess, 142.0ms inference, 26.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 2.9532909393310547, 'inference': 142.0137882232666, 'postprocess': 26.135921478271484}]\n",
      "\n",
      "ModÃ¨le : yolo11s_saved_model\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11s_saved_model for TensorFlow SavedModel inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 577.3ms\n",
      "Speed: 3.1ms preprocess, 577.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 3.147602081298828, 'inference': 577.3284435272217, 'postprocess': 1.1637210845947266}]\n",
      "\n",
      "ModÃ¨le : yolo11s.pb\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11s.pb for TensorFlow GraphDef inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 1287.2ms\n",
      "Speed: 3.1ms preprocess, 1287.2ms inference, 29.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 3.0808448791503906, 'inference': 1287.2354984283447, 'postprocess': 29.439210891723633}]\n",
      "\n",
      "ModÃ¨le : yolo11s_saved_model/yolo11s_float32.tflite\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11s_saved_model/yolo11s_float32.tflite for TensorFlow Lite inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 310.8ms\n",
      "Speed: 7.8ms preprocess, 310.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[119, 146, 172],\n",
      "        [121, 148, 174],\n",
      "        [122, 152, 177],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[120, 147, 173],\n",
      "        [122, 149, 175],\n",
      "        [123, 153, 178],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       [[123, 150, 176],\n",
      "        [124, 151, 177],\n",
      "        [125, 155, 180],\n",
      "        ...,\n",
      "        [161, 171, 188],\n",
      "        [160, 170, 187],\n",
      "        [160, 170, 187]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[183, 182, 186],\n",
      "        [179, 178, 182],\n",
      "        [180, 179, 183],\n",
      "        ...,\n",
      "        [121, 111, 117],\n",
      "        [113, 103, 109],\n",
      "        [115, 105, 111]],\n",
      "\n",
      "       [[165, 164, 168],\n",
      "        [173, 172, 176],\n",
      "        [187, 186, 190],\n",
      "        ...,\n",
      "        [102,  92,  98],\n",
      "        [101,  91,  97],\n",
      "        [103,  93,  99]],\n",
      "\n",
      "       [[123, 122, 126],\n",
      "        [145, 144, 148],\n",
      "        [176, 175, 179],\n",
      "        ...,\n",
      "        [ 95,  85,  91],\n",
      "        [ 96,  86,  92],\n",
      "        [ 98,  88,  94]]], dtype=uint8)\n",
      "orig_shape: (1080, 810)\n",
      "path: '/home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/classify/predict'\n",
      "speed: {'preprocess': 7.800817489624023, 'inference': 310.7783794403076, 'postprocess': 1.0213851928710938}]\n",
      "\n",
      "ModÃ¨le : yolo11s_paddle_model\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11s_paddle_model for PaddlePaddle inference...\n"
     ]
    }
   ],
   "source": [
    "for model in models_path:\n",
    "    print(f\"\\nModÃ¨le : {model}\")\n",
    "    model = YOLO(model)\n",
    "    results = model(\"https://ultralytics.com/images/bus.jpg\", task=\"classify\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n_ncnn_model for NCNN inference...\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/jake/Projet_developpement_logiciel_application_IA_embarquee/yolo/bus.jpg: 640x640 4 persons, 1 bus, 92.8ms\n",
      "Speed: 25.8ms preprocess, 92.8ms inference, 23.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "ncnn_model = YOLO(\"yolo11n_ncnn_model\")\n",
    "\n",
    "# Run inference\n",
    "results = ncnn_model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_developpement_application_IA_embarquee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
