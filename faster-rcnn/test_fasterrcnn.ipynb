{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Rcnn.pt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SONG\\anaconda3\\envs\\transformer\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\SONG\\anaconda3\\envs\\transformer\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\SONG\\AppData\\Local\\Temp\\ipykernel_21944\\1743769785.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"fasterrcnn_custom_new.pt\",map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 8  \n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=NUM_CLASSES)\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(torch.load(\"fasterrcnn_custom_new.pt\",map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Move model to CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to change json path here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata from ../data/test/result.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file = \"../data/test/result.json\"  \n",
    "try:\n",
    "    with open(json_file, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "        print(f\"Loaded metadata from {json_file}.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: JSON file not found at {json_file}.\")\n",
    "    raise\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON file: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory set to ./predictions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "output_dir = \"./predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory set to {output_dir}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 1: images/194e378c-608.png (Inference time: 2.1900 seconds)\n",
      "Processed image 2: images/fcfa48ab-589.png (Inference time: 2.0834 seconds)\n",
      "Processed image 3: images/986425fd-590.png (Inference time: 2.2694 seconds)\n",
      "Processed image 4: images/2d82eb4c-591.png (Inference time: 2.1666 seconds)\n",
      "Processed image 5: images/9ef8c916-592.png (Inference time: 2.1483 seconds)\n",
      "Predictions saved to ./predictions/predictions_i5-8265U_fasterrcnn.pt.json.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import time\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for idx, image_meta in enumerate(metadata.get(\"images\", [])[:5]):  \n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        image_path = os.path.join(\"../data/test\", image_meta[\"file_name\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        input_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        outputs = model(input_tensor)\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "        # Post-process outputs\n",
    "        for box, label, score in zip(\n",
    "            outputs[0]['boxes'].tolist(),\n",
    "            outputs[0]['labels'].tolist(),\n",
    "            outputs[0]['scores'].tolist()\n",
    "        ):\n",
    "            if score > 0.5:  # Confidence threshold\n",
    "                bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]  # Convert to [x, y, w, h]\n",
    "                predictions.append({\n",
    "                    \"image_id\": image_meta[\"id\"],\n",
    "                    \"category_id\": int(label),\n",
    "                    \"bbox\": [float(coord) for coord in bbox],  # Convert to float\n",
    "                    \"score\": float(score),  # Convert to float\n",
    "                    \"inference_time\": inference_time  # Store inference time\n",
    "                })\n",
    "\n",
    "        print(f\"Processed image {idx + 1}: {image_meta['file_name']} (Inference time: {inference_time:.4f} seconds)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_meta['file_name']}: {e}\")\n",
    "\n",
    "# Save predictions to JSON\n",
    "output_json = \"./predictions/predictions_i5-8265U_fasterrcnn.pt.json\"\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=4)\n",
    "print(f\"Predictions saved to {output_json}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ONNX model from fasterrcnn_custom_new.onnx.\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "# Load the ONNX model once before the loop\n",
    "onnx_model_path = \"fasterrcnn_custom_new.onnx\"\n",
    "try:\n",
    "    session = onnxruntime.InferenceSession(onnx_model_path)  # Open session once\n",
    "    print(f\"Loaded ONNX model from {onnx_model_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ONNX model: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1/178: images/194e378c-608.png\n",
      "Processed image images/194e378c-608.png with 2 detections.\n",
      "Processing image 2/178: images/fcfa48ab-589.png\n",
      "Processed image images/fcfa48ab-589.png with 2 detections.\n",
      "Processing image 3/178: images/986425fd-590.png\n",
      "Processed image images/986425fd-590.png with 4 detections.\n",
      "Processing image 4/178: images/2d82eb4c-591.png\n",
      "Processed image images/2d82eb4c-591.png with 4 detections.\n",
      "Processing image 5/178: images/9ef8c916-592.png\n",
      "Processed image images/9ef8c916-592.png with 5 detections.\n",
      "Predictions saved to ./predictions\\predictions_i5-8265U_fasterrcnn.onnx.json.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import functional as F\n",
    "import time  # Import time module for measuring inference time\n",
    "\n",
    "\n",
    "# Preprocess and predict on each image\n",
    "predictions = []\n",
    "for idx, image_meta in enumerate(metadata.get(\"images\", [])[:5]):  # Process only the first 5 images for testing):\n",
    "    try:\n",
    "        # Get image path\n",
    "        image_path = os.path.join(\"../data/test\", image_meta[\"file_name\"])\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image not found at {image_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        print(f\"Processing image {idx + 1}/{len(metadata['images'])}: {image_meta['file_name']}\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # Resize the image to 224x224\n",
    "        resized_image = image.resize((224, 224))\n",
    "        input_tensor = F.to_tensor(resized_image).unsqueeze(0).numpy()\n",
    "\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        ort_inputs = {session.get_inputs()[0].name: input_tensor}\n",
    "        ort_outs = session.run(None, ort_inputs)\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "        # Post-process the output\n",
    "        boxes, labels, scores = ort_outs\n",
    "        boxes, labels, scores = ort_outs\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if score > 0.5:  # Apply confidence threshold\n",
    "                # Convert box format [x1, y1, x2, y2] to [x, y, width, height]\n",
    "                bbox = [float(box[0]), float(box[1]), float(box[2] - box[0]), float(box[3] - box[1])]\n",
    "                predictions.append({\n",
    "                    \"image_id\": image_meta[\"id\"],\n",
    "                    \"category_id\": int(label),  # Convert to int\n",
    "                    \"bbox\": bbox,\n",
    "                    \"score\": float(score),  # Convert to float\n",
    "                    \"inference_time\": float(inference_time)  # Convert to float\n",
    "                })\n",
    "\n",
    "        print(f\"Processed image {image_meta['file_name']} with {len(predictions)} detections.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_meta['file_name']}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save results to a JSON file\n",
    "output_json = os.path.join(output_dir, \"predictions_i5-8265U_fasterrcnn.onnx.json\")\n",
    "try:\n",
    "    with open(output_json, \"w\") as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "        print(f\"Predictions saved to {output_json}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving predictions to {output_json}: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct the bounding boxes in JSON file\n",
    " back to the dimensions of the original image (640x640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected predictions saved to ./predictions/predictions_RTX 3080 TI laptop 95W_fasterrcnn_custom_new.onnx.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "json_file_path = \"./predictions/predictions_i5-8265U_fasterrcnn.onnx.json\"\n",
    "original_width = 640\n",
    "original_height = 640\n",
    "resized_width = 224\n",
    "resized_height = 224\n",
    "\n",
    "scale_factor_x = original_width / resized_width\n",
    "scale_factor_y = original_height / resized_height\n",
    "\n",
    "# Load predictions\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "# Correct bounding boxes\n",
    "for prediction in predictions:\n",
    "    bbox = prediction[\"bbox\"]  # [x, y, width, height]\n",
    "    prediction[\"bbox\"] = [\n",
    "        bbox[0] * scale_factor_x,        # Scale x\n",
    "        bbox[1] * scale_factor_y,        # Scale y\n",
    "        bbox[2] * scale_factor_x,        # Scale width\n",
    "        bbox[3] * scale_factor_y         # Scale height\n",
    "    ]\n",
    "\n",
    "# Save corrected predictions back to JSON\n",
    "output_corrected_json = \"./predictions/predictions_i5-8265U_fasterrcnn.onnx.json\"\n",
    "with open(output_corrected_json, \"w\") as f:\n",
    "    json.dump(predictions, f, indent=4)\n",
    "\n",
    "print(f\"Corrected predictions saved to {output_corrected_json}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
