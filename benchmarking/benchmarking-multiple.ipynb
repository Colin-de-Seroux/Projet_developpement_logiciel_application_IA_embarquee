{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking pour un dossier de prédictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Author Colin de Seroux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue\">Installation des dépendances</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue\">Importation des dépendances</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from utils import extract_annotations_from_file, extract_predictions_from_folder, extract_metrics, get_coco_predictions, evaluate_metrics, extract_inference_time, plot_inference_time, plot_inference_time_mAP_50_95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue\">Code principale</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">Environnement</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_FILE_PATH = \"../data/dataset4/test/result.json\"\n",
    "PREDICTIONS_DIR = \"../results/predictions\"\n",
    "DATASET_NAME = \"DATASET 4\"\n",
    "COMPARED_TYPE = f\"{DATASET_NAME} YOLO\"\n",
    "INFERENCE_TIME_GOAL = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">Récupération des données</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, class_names = extract_annotations_from_file(ANNOTATIONS_FILE_PATH)\n",
    "y_preds, y_scores = extract_predictions_from_folder(ANNOTATIONS_FILE_PATH, PREDICTIONS_DIR)\n",
    "coco_annotations = COCO(ANNOTATIONS_FILE_PATH)\n",
    "coco_predictions = get_coco_predictions(coco_annotations, PREDICTIONS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">mAP</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialiser l'évaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_evals = []\n",
    "\n",
    "for coco_prediction in coco_predictions:\n",
    "    coco_eval = COCOeval(coco_annotations, coco_prediction, iouType=\"bbox\")\n",
    "    coco_evals.append(coco_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculer les AP et AR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coco_eval in coco_evals:\n",
    "\tcoco_eval.evaluate()\n",
    "\tcoco_eval.accumulate()\n",
    "\tcoco_eval.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">Récupération des metrics</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for coco_eval in coco_evals:\n",
    "    precision, recall, scores, iou_lookup = evaluate_metrics(coco_eval)\n",
    "    current_metrics = extract_metrics(precision, recall)\n",
    "    metrics.append(current_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">Traitement des données</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_files, inference_times, average_inference_time = extract_inference_time(PREDICTIONS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">Comparaison uniquement sur le temps</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inference_time(prediction_files, inference_times, average_inference_time, INFERENCE_TIME_GOAL, COMPARED_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">Nuage de points pour une comparaison simplifiée</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inference_time_mAP_50_95(prediction_files, inference_times, average_inference_time, metrics, INFERENCE_TIME_GOAL, DATASET_NAME, COMPARED_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:lightgreen\">Comparaison sous forme de tableau avec le F1-score</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for metric, inference_time, prediction_file in zip(metrics, inference_times, prediction_files):\n",
    "    data.append({\n",
    "        \"Nom du modèle\": prediction_file,\n",
    "        \"mAP 50-95\": metric[\"mAP_50_95\"],\n",
    "        \"F1-score\": metric[\"f1_score_50_95\"],\n",
    "        \"Temps d'inférence moyen\": inference_time\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(data)\n",
    "\n",
    "df_metrics.style.format({\n",
    "    \"mAP 50-95\": \"{:.3}\".format,\n",
    "    \"F1-score\": \"{:.3}\".format,\n",
    "    \"Temps d'inférence moyen\": \"{:.4}ms\".format\n",
    "})\n",
    "\n",
    "df_metrics.to_csv(f\"../results/benchmarking/{COMPARED_TYPE}_metrics.csv\", index=False)\n",
    "\n",
    "df_metrics = df_metrics.sort_values(by=\"Temps d'inférence moyen\")\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:lightblue\">Sources</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://gist.github.com/shivamsnaik/c5c5e99c00819d2167317b1e56871187\n",
    "- https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.f1_score.html\n",
    "- https://kobia.fr/classification-metrics-f1-score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
